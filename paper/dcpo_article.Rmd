---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: svm-latex-ms2.tex
title: "Modeling Dynamic Comparative Public Opinion [DRAFT]"
thanks: "Prepared for the 2020 meeting of the Southern Political Science Association.  The paper's revision history and the materials needed to reproduce its analyses can be found [on Github here](http://github.com/fsolt/dcpo_article).  I am grateful for comments received on previous versions of this project at the 2014 meetings of the European Political Science Association and American Political Science Association, the 2016 meeting of the Midwest Political Science Association, and at Princeton University, Oxford University, the University of Tennessee, the University of Iowa, Central European University, and the Polish Academy of Sciences.  Corresponding author: [frederick-solt@uiowa.edu](mailto:frederick-solt@uiowa.edu). Current version: `r format(Sys.time(), '%B %d, %Y')`."
author:
- name: Frederick Solt
  affiliation: University of Iowa
abstract: "The study of public opinion in comparative context has been hampered by data that is sparse, that is, unavailable for many countries and years; incomparable, i.e., ostensibly addressing the same issue but generated by different survey items; or, most often, both.  Questions of representation and of policy feedback on public opinion, for example, cannot be explored fully from a cross-national perspective without comparable time-series data for many countries that span their respective times of policy adoption.  Recent works (Claassen 2019; Caughey, O'Grady, and Warshaw 2019) have introduced a latent variable approach to the study of comparative public opinion that maximizes the information gleaned from available surveys to overcome issues of sparse and incomparable data and allow comparativists to examine the dynamics of public opinion.  This paper advances this field of research by presenting a new model and software for estimating latent variables of public opinion from cross-national survey data that yield superior fit and more quantities of theoretical interest than previous works allow."
keywords: "public opinion, item response theory, measurement"
date: "`r format(Sys.time(), '%B %d, %Y')`"
fontsize: 11pt
spacing: double
bibliography: \dummy{`r file.path(getwd(), list.files(getwd(), ".bib$"))`}
biblio-style: apsr
citecolor: black
linkcolor: black
endnote: no
header-includes:
      - \usepackage{array}
      - \usepackage{caption}
      - \usepackage{graphicx}
      - \usepackage{siunitx}
      - \usepackage{colortbl}
      - \usepackage{multirow}
      - \usepackage{hhline}
      - \usepackage{calc}
      - \usepackage{tabularx}
      - \usepackage{threeparttable}
      - \usepackage{wrapfig}
nocite: |
  @Solt2020a
---

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
# load all the packages you will use below 
library(huxtable)
library(kableExtra)
library(DCPOtools)
library(tidyverse)
library(latex2exp)
```

A perennial problem confronting scholars of comparative public opinion is that the exact question relevant to their research topic is only infrequently asked, making response data scarce across countries and over time.
Although a wealth of surveys may provide information on the state of public opinion on the issue of interest in different countries over the years, significant hurdles exist to putting all of this information to use in any comparative study, beginning with the incomparability of the multitude of different questions asked.

As a result, much work in the study of comparative public opinion examines one cross-section of countries, typically provided by a single cross-national survey [see, e.g., @Ansell2014; @Clayton2019].
Some studies capture some element of change over time by taking advantage of multiple waves of an ongoing cross-national survey [see, e.g., @Abou-Chadi2019; @Busemeyer2020] or, less commonly, drawing on multiple surveys that employed the same items [see, e.g., @Ezrow2014; @Dalton2017].
Only for a very few topics are scholars able to capitalize on a relevant item that has been frequently asked by the same survey in many countries: see, for example, how @Norris2011 [, 70-77] took advantage of the Eurobarometer's longstanding questions on trust in government, and @Hagemann2017 that program's equally longstanding item on whether European integration is a "good thing." 

A growing body of work takes a different tack, examining the dynamics of public opinion in one or a few countries over time.
One approach of this sort compiles responses to the same survey questions asked on an annual or near-annual basis [see, e.g., @Hobolt2008; @Soroka2010].
But even narrowing one's focus to only a single country typically gains few items asked with such regularity, and concerns that any single question can adequately capture opinion are well placed. 
These factors have led to research applying the dyad-ratios algorithm elaborated in @Stimson1991, tremendously influential in the study of American politics, beyond the U.S. case: among others, @Bellucci2017 on Italian policy mood and @Jennings2017 on British political discontent.

In recent years, there have even been applications of Stimson's dyad-ratios method to each of many countries.
@Carlin2018, for example, generate time series of presidential approval in eighteen Latin American countries, and @Guinaudeau2019 generate annual measures of attitudes toward European integration in the countries of the European Union.
A shortcoming of this approach, however, is that because each country's series is estimated separately, the results are not assured to be comparable across countries.

<!-- field of comparative public opinion -->
<!-- lack of dynamics (in contrast to public opinion work in U.S.) -->
<!-- Check out -->
<!-- 	Thomassen2011 -->

<!-- question of feedback: positive or negative? ("policies create constituencies" vs. thermostatic) under what conditions? See also Busemeyer2020-->

Latent variable models estimated with Bayesian methods provide a means of overcoming this problem, and they have become a very useful tool for combining the information available in disparate sources into cross-nationally comparable time series. 
Such models have been used to measure many difficult concepts in comparative politics, from democracy [@Treier2008; @Pemstein2010] through governance [@Arel-Bundock2011] and state capacity [@Hanson2019] to judicial independence [@Linzer2015] and respect for human rights [@Fariss2014].
Two recent works, @Claassen2019 and @Caughey2019, have introduced a Bayesian latent variable approach to the study of comparative public opinion that maximizes the information gleaned from available surveys to overcome issues of sparse and incomparable data and allow comparativists to examine the dynamics of public opinion.

This paper presents DCPO, a new model and software for estimating Bayesian latent variables of comparative dynamic public opinion from cross-national survey data.
The DCPO model yields superior fit and more quantities of theoretical interest than the models offered in previous works allow.
The `DCPO` package for R not only estimates this model, but also facilitates the formidable data management tasks involved: collecting and managing the survey datasets, identifying the country-years in which respondents were contacted in each, and preparing the response data for the model.

The paper begins with a detailed description of the DCPO model, building up from the individual level.
It continues with a comparison of the performance of the DCPO model to the alternate models of public opinion presented in @Claassen2019 and @Caughey2019.
The paper then describes the workflow involved in generating estimates of dynamic comparative public opinion with the `DCPO` software and employing them in analyses.


# Estimating Dynamic Comparative Public Opinion

The logic underlying DCPO's approach to estimating dynamic comparative public opinion starts at the individual level with the two-parameter logistic item response theory ("2PL IRT") model.  In this model, the probability that individual $i$ responds affirmatively to a dichotomous question $q$ is a function of the individual's score on the unbounded latent trait, $\theta'_{i}$, plus two parameters that characterize the question, its *difficulty*, $\beta_{q}$, and its *dispersion*, $\alpha_{q}$:
\begin{equation}
\textrm{Pr}(y_{iq} = 1) = \textrm{logit}^{-1}(\frac{\theta'_{i} - \beta_{q}}{\alpha_{q}}) \label{eq:irt}
\end{equation}

Figure \ref{fig:irt} shows how these parameters interact using simulated data.
First, both panels show that regardless of the question, as an individual's score on the unbounded latent trait, $\theta'_{i}$, increases, the individual becomes more likely to give an affirmative response.
Second, the left panel describes how the probability of answering affirmatively changes with question difficulty, $\beta_{q}$: some questions demand more of respondents than others.
Take, for example, questions regarding abortion.
One need not hold views very favorable toward abortion rights to agree that abortion should be permitted when the life of the woman is at risk.
The question is not very difficult, and only those who view abortion very unfavorably will answer negatively.
To respond affirmatively to a question that asks whether abortion should be permitted for a single woman who does not want to marry the man requires respondents to have more favorable views toward abortion; this question is more difficult.
The question of whether one *approves* of abortion in this same situation is still more difficult, that is, some people who accept that abortion should be permitted in such cases will nevertheless express disapproval of women who exercise their right.
The left panel illustrates this graphically: it shows that individuals with the same score on the latent trait are less likely to respond affirmatively to a question as the question's difficulty increases (here, all three questions depicted have a dispersion of 1).
When $\theta'_{i}$ and $\beta_{q}$ are equal, the probability of an affirmative response is 50%. 

```{r irt, fig.width=6, fig.height=3, fig.cap="\\label{fig:irt}Probability of an Affirmative Response in the 2PL IRT Model"}
tibble(theta = rep(seq(-3, 3, length.out = 100), 6),
       beta = rep(c(-2, 0, 2, 0, 0, 0), each = 100),
       alpha = rep(c(1, 1, 1, .25, 1, 2), each = 100),
       pr_y = plogis((theta - beta)/alpha),
       line_no = rep(1:6, each = 100),
       plot_facet = rep(c("Varying Question Difficulty", "Varying Question Dispersion"), each = 300)) %>% 
    ggplot(aes(theta, pr_y, group = line_no)) +
    geom_line() +
    facet_wrap( ~ plot_facet) +
    xlab(TeX("Individual Latent Trait")) +
    ylab(TeX("Pr(y_{iq} = 1)")) +
    theme_bw() +
    theme(strip.background = element_rect(colour="white", fill="white"),
          plot.title.position = "plot") +
    scale_x_continuous(breaks = seq(-3, 3, 1)) +
    geom_text(data = tibble(theta = -2.25,
                            pr_y = .65,
                            line_no = 1,
                            plot_facet = "Varying Question Difficulty",),
             label = TeX("$\\beta_1 = -2$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = -.6,
                            pr_y = .525,
                            line_no = 1,
                            plot_facet = "Varying Question Difficulty",),
              label = TeX("$\\beta_2 = 0$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = .9,
                            pr_y = .4,
                            line_no = 1,
                            plot_facet = "Varying Question Difficulty",),
              label = TeX("$\\beta_3 = 2$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = 2.5,
                            pr_y = 0.025,
                            line_no = 1,
                            plot_facet = "Varying Question Difficulty",),
              label = TeX("$\\alpha_q = 1$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = -.03,
                            pr_y = .96,
                            line_no = 1,
                            plot_facet = "Varying Question Dispersion",),
             label = TeX("$\\alpha_1 = .25$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = 1.4,
                            pr_y = .9,
                            line_no = 1,
                            plot_facet = "Varying Question Dispersion",),
              label = TeX("$\\alpha_2 = 1$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = 2.1,
                            pr_y = .66,
                            line_no = 1,
                            plot_facet = "Varying Question Dispersion",),
              label = TeX("$\\alpha_3 = 2$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = 2.5,
                            pr_y = 0.025,
                            line_no = 1,
                            plot_facet = "Varying Question Dispersion",),
              label = TeX("$\\beta_q = 0$", output = "character"), parse = TRUE)

```

Third, the right panel of Figure \ref{fig:irt} portrays how the *dispersion* of the question, $\alpha_{q}$, affects the probability of positive responses.
A question's dispersion is proportional to its measurement error with regard to the latent variable; it may be larger because the question is confusing to some respondents, or because the question does not cleanly map onto the latent trait.
For example, a question that asks respondents about laws protecting a population from employment discrimination may tap views about workplace protections as much or more than it does attitudes toward the particular group.
If it is nevertheless used as an indicator of the latter, this question will have a high dispersion.
As shown in the right panel, as question dispersion increases, individuals with lower scores on the latent trait become more likely to respond affirmatively and those with higher scores on the latent trait are more likely to respond negatively; in other words, the slope of the curve describing the relationship between the latent trait and the probability of answering affirmatively flattens out (here, all three questions depicted have a difficulty of 0).

To aggregate individual-level responses to generate a population-level estimate of public opinion is a matter of integration.
If scores on the unbounded latent trait are modeled as normally distributed within the population of country $k$ at time $t$, integrating Equation \ref{eq:irt} yields the population-level two-parameter logistic IRT model:
\begin{equation}
\eta_{ktq} = \textrm{logit}^{-1}(\frac{\bar{\theta'}_{kt} - \beta_{q}}{\sqrt{\alpha_{q}^2 + (1.7*\sigma_{kt})^2}}) \label{eq:pop_irt}
\end{equation}
where $\eta_{ktq}$ is the expected probability that a random person in country $k$ at time $t$ answers question $q$ affirmatively and $\bar{\theta'}_{kt}$ and $\sigma_{kt}$ are the mean and standard deviation of the unbounded latent trait $\theta'$ in the population of country $k$ at time $t$ [see @Mislevy1983, 280; see also, using the two-parameter probit IRT model, @McGann2014, 120].

Further, many survey questions are not simply dichotomous, but instead measure respondents' attitudes on ordinal scales, assumed here to range from 1 to $R$.
To take advantage of this additional information, DCPO uses the cumulative logit formulation, in which the probability to be estimated is not that individual $i$ provided a simple affirmative response, as in Equation \ref{eq:pop_irt} above, but instead the probability that individual $i$ provided a response at least as positive as response $r$ for all $r$ greater than 1 and less than or equal to $R$.
This yields a population-level graded response model:
\begin{equation}
\eta_{ktqr} = \textrm{logit}^{-1}(\frac{\bar{\theta'}_{kt} - \beta_{qr}}{\sqrt{\alpha_{q}^2 + (1.7*\sigma_{kt})^2}}) \label{eq:pop_grm}
\end{equation}

Again, the differences between Equation \ref{eq:pop_grm} for the population-level graded response model and Equation \ref{eq:pop_irt} for the population-level two-parameter logistic IRT model above are in the additional subscripts for $r$ to $\eta$ and $\beta$.
In Equation \ref{eq:pop_grm}, $\eta_{ktqr}$ is the expected probability that a random individual in country $k$ at time $t$ replies to question $q$ with a response at least as positive as response $r$.
And the additional subscript to $\beta_{qr}$ indicates that this parameter represents the difficulty of response $r$ of question $q$, constrained to be increasing for increasing $r$. 
That is to say, for example, that the model makes the straightforward assumption that the response of "strongly agree" to a question will, on average across respondents, reflect higher levels of the latent trait than the response of "agree" to that same question.

A final addition to the DCPO model of the probability $\eta_{ktqr}$ takes into account differences in item response bias across countries.
Responses to survey questions may vary across different countries not only as a result of differences in attitudes and preferences but also due to translation issues [see, e.g., @Davidov2010], cultural differences in acquiescence and extreme response styles [see, e.g., @VanHerk2004], or other idiosyncrasies---recall Tarrow's [-@Tarrow1971, 344] famous observation that the French understood survey questions regarding their 'interest in politics' as inquiring about the strength of their partisan affiliations.
Rather than simply allowing such problems of equivalence to contribute to the error of the model, they can be addressed in part by explicitly modeling the country-specific item bias  [@Stegmueller2011].
In the context of the population-level graded response model presented in Equation \ref{eq:pop_grm}, this can be done by including country-specific item bias as a country-varying shift in the difficulty of each question, denoted here as $\delta_{kq}$:
\begin{equation}
\eta_{ktqr} = \textrm{logit}^{-1}(\frac{\bar{\theta'}_{kt} - (\beta_{qr} + \delta_{kq})}{\sqrt{\alpha_{q}^2 + (1.7*\sigma_{kt})^2}}) \label{eq:dcpo}
\end{equation}
Estimating $\delta_{kq}$ in Equation \ref{eq:dcpo} requires repeated administrations of question $q$ in country $k$.
When responses to question $q$ are observed in country $k$ in only a single year, the DCPO model sets $\delta_{kq}$ to zero by assumption.
This means that incorporating these 'one-shot' surveys comes at the price of increasing the error of the model by any country-item bias that is present.^[
Researchers uncomfortable with this price may follow Claassen's [-@Claassen2019, 5-6] approach with regard to country-specific item bias, which is to instead discard survey data on questions that were not administered more than once in a given country at the cost of giving up all of the information contained in these data to guard against the mere risk of introducing some additional error to the estimates.]
Of course, questions that are asked repeatedly over time in only a single country pose no risk of country-specific item bias, so $\delta_{kq}$ in such cases can be set to zero without harm.

Before continuing, it is useful to pause to briefly review the relationships of the parameters in this equation.
The numerator of Equation \ref{eq:dcpo} implies first that $\eta_{ktqr}$, the expected probability of a response at least as positive as $r$, increases as the mean value of the unbounded latent trait in the population, $\bar{\theta'}_{kt}$, increases and second that $\eta_{ktqr}$ decreases with more difficult questions and higher response categories, $\beta_{qr}$, and particularly so where the item-response bias, $\delta_{kq}$, is positive.
The equation's denominator implies that $\eta_{ktqr}$ is drawn closer to 50% as the geometric mean of the question's dispersion, $\alpha_{q}$, and the standard deviation in opinion in the population, $\sigma_{kt}$, increases: individuals sampled even from a population with a negative mean value on the unbounded latent trait and answering a difficult question will still be more likely to supply more positive responses---and those from a population with a positive mean score on the unbounded latent trait and answering an easy question will be more likely to supply more negative responses---if the question's dispersion is greater or attitudes are more polarized in the population.

Given this expected probability $\eta_{ktqr}$, the total number of survey responses at least as positive as $r$ to each question $q$ in country $k$ at time $t$, $y_{ktqr}$, out of the total number of respondents surveyed, $n_{ktqr}$, is then modeled using the beta-binomial distribution, which allows for an overall dispersion parameter, $\phi$, to account for additional sources of survey error [see @McGann2014, 120; @Claassen2019, 4-5].
\begin{equation}
a_{ktqr} = \phi\eta_{ktqr} \label{eq:bb_a}
\end{equation}
\begin{equation}
b_{ktqr} = \phi(1 - \eta_{ktqr}) \label{eq:bb_b}
\end{equation}
\begin{equation}
y_{ktqr} \sim \textrm{BetaBinomial}(n_{ktqr}, a_{ktqr}, b_{ktqr}) \label{eq:betabinomial}
\end{equation}

Then, to account for the dynamics of comparative public opinion---the change over time---the prior distributions for the public opinion parameters in Equation \ref{eq:dcpo}, $\bar{\theta'}_{kt}$ and $\sigma_{kt}$, are given by simple local-level dynamic linear models.
For $\bar{\theta'}_{kt}$, this random-walk prior takes the following form:
\begin{equation}
\bar{\theta'}_{kt} \sim \textrm{N}(\bar{\theta'}_{k,t-1}, \sigma_{\bar{\theta'}}^2) \label{eq:prior_theta}
\end{equation}
By treating these parameters' values at time $t-1$ as their expected values at time $t$, this prior works to smooth estimates of the mean of the latent trait in the population of each country over time.
For $\sigma_{kt}$, a lognormally distributed prior is used:
\begin{equation}
\sigma_{kt} \sim \textrm{LN}(\textrm{log}(\sigma_{k,t-1}), \sigma_{\sigma}^2) \label{eq:prior_sigma}
\end{equation}
The lognormal distribution ensures that the prior for $\sigma_{kt}$ takes on only positive values.
In both cases, if no survey data is available for a particular time, these dynamic models provide estimates based on the estimates for previous and subsequent periods.
The variances $\sigma_{\bar{\theta'}}^2$ and $\sigma_{\sigma}^2$ are estimated from the data.

Previous efforts to measure cross-national aggregate public opinion as a latent variable have generated estimates on an unbounded scale with mean zero and unit variance [see @Claassen2019, 14; @Caughey2019, 8], and this is the scale of $\bar{\theta'}_{kt}$.
However, like many other concepts in political science [see @Linzer2015, 229], many aspects of public opinion are conceptually bounded, that is, it make sense to think of them as lying along a scale from fully absent to fully present in the relevant public.
Take attitudes toward immigration as an example.
One hypothetical country's citizens are, at a given time, absolutely opposed to any immigration, while another's are completely welcoming to migrants.
Actual countries' levels of public opinion toward immigration are better understood not as unbounded but as falling somewhere between the bounds described by these two hypothetical countries.
It is not surprising, then, that earlier works aggregating public opinion within a single country presented results on bounded scales [see @Stimson1991; @McGann2014].
Even if the issue in question is less easily viewed as bounded, bounding is still a good idea because it reduces the uncertainty for the estimates for countries at the extremes, that is, those countries whose values should in fact be easier to estimate.
As Linzer and Staton [-@Linzer2015, 229] note, "bounding the latent variable may do little harm to the scale and produce more sensible estimates of uncertainty."
DCPO therefore uses the logistic function to transform the unbounded estimates, $\bar{\theta'}_{kt}$, to the unit interval:
\begin{equation}
\bar{\theta}_{kt} = \textrm{logit}^{-1}({\bar{\theta'}_{kt}} - 1)
\end{equation}
The resulting $\bar{\theta}_{kt}$ is the DCPO estimate of the mean public opinion for country $k$ at time $t$.
Together, $\sigma_{kt}$, which provides a measure of polarization in public opinion, and $\bar{\theta}_{kt}$ will typically be the main quantities of interest.

The model is identified by imposing a series of constraints that fix location, direction, and scale.
The dispersion parameters $\alpha$ are constrained to be positive---and all survey responses are coded to have the same polarity---to fix direction.
One researcher-specified difficulty parameter $\beta$ is set to a value of 1 to identify location, and, as mentioned previously, for each question $q$ the difficulties for increasing response values $r$ are constrained to be increasing.
To ensure identification, the sum of $\delta_{kq}$ across all countries $k$ is fixed to zero for each question $q$:
\begin{equation}
\sum_{k = 1}^K \delta_{kq} = 0
\end{equation}

Weakly informative priors are placed on most parameters.
The dispersion parameters $\alpha_{q}$ are drawn from standard half-normal prior distributions, that is, the positive half of N(0, 1).
The first difficulty parameters for each question, $\beta_{q1}$, are drawn from standard normal prior distributions, and the differences between $\beta$s for each $r$ for the same question $q$ are drawn from standard half-normal prior distributions.
The item-bias parameters $\delta_{kq}$ receive normally-distributed hierarchical priors with mean 0 and standard deviations drawn from standard half-normal prior distributions.
The initial value of the mean unbounded latent trait for each country, $\bar{\theta'}_{k1}$, is assigned a standard normal prior, as are the transition variances $\sigma_{\bar{\theta'}}^2$ and $\sigma_{\sigma}^2$; the initial value of the standard deviation of the unbounded latent trait for each country, $\sigma_{k1}$, is drawn from a standard lognormal prior distribution.
The overall dispersion, $\phi$, receives a somewhat more informative prior drawn from a gamma(4, 0.1) distribution that yields values that are well scaled for that parameter.
The model is estimated using the `DCPO` package for R [@Solt2020a], which is written in the Stan probabilistic programming language [@StanDevTeam2019a; @StanDevTeam2019b].


```{r comparison_table}
comparison_data <- tibble::tribble(
    ~` `, ~`McGann (2014)`, ~`\\vtop{\\hbox{\\strut Claassen (2019)}\\hbox{\\strut \\quad \\, Model 5}}`, ~`\\vtop{\\hbox{\\strut Caughey, O'Grady,}\\hbox{\\strut and Warshaw (2019)}}`, ~DCPO,
    "\\vtop{\\hbox{\\strut Cross-National}\\hbox{\\strut }}", "No", "Yes", "Yes", "Yes",
    "\\vtop{\\hbox{\\strut Dynamic Priors}\\hbox{\\strut }}", "No", "Yes", "Yes", "Yes",
    "\\vtop{\\hbox{\\strut Ordinal}\\hbox{\\strut }}", "No", "No", "Yes", "Yes",
    "\\vtop{\\hbox{\\strut Country-Varying}\\hbox{\\strut Question Difficulty}}", "No", "Yes", "No", "Yes",
    "\\vtop{\\hbox{\\strut Bounded Mean}\\hbox{\\strut Public Opinion}}", "Yes", "No", "No", "Yes",
    "\\vtop{\\hbox{\\strut Country-Year Public}\\hbox{\\strut Opinion Std.~Deviation}}", "Yes", "No", "No", "Yes"
)

comparison_ht <- comparison_data %>%
  as_hux() %>% 
  add_colnames() %>% 
  set_width(.8) %>%
  set_bold(1, 1:5, TRUE) %>% 
  set_bold(everywhere, final(1), TRUE) %>% 
  set_top_border(1, everywhere, 1) %>%
  set_bottom_border(1, 1:5, 1) %>%
  set_bottom_border(final(1), everywhere, 1) %>% 
  set_caption('Comparing Latent Variable Models of Public Opinion') %>% 
  set_position("left") %>% 
  set_escape_contents(everywhere, everywhere, FALSE) %>% 
  set_background_color(evens, everywhere, "grey95") %>% 
  set_align(-1, -1, "center") %>% 
  set_label("tab:comparison_table")

comparison_ht

```

Table \ref{tab:comparison_table} summarizes the DCPO model just described in comparison to three alternate ways of modeling public opinion: that presented by @McGann2014, Claassen's [-@Claassen2019] preferred Model 5, and that employed in @Caughey2019.
It reveals that DCPO incorporates the desirable features from each of those three models.
DCPO generates estimates of public opinion that are comparable not only over time but also across countries, takes advantage of all of the information encoded in responses to ordinal questions, explicitly accounts for country-item bias by including terms to capture country-varying question difficulty, provides easily interpreted bounded estimates of mean public opinion, and includes the standard deviation of public opinion by country-year to represent the extent of polarization.
The next section provides a demonstration of how these features allow DCPO to provide a better fit to actual data.


# Relative Fit
To assess the ability of the DCPO model to fit public opinion data relative to the two alternative approaches, I use the set of survey questions on support for democracy employed in @Claassen2019 [, 7-8].

The first three columns of Table \ref{tab:validation_table} present the results of an internal validation test, that is, a test that uses the same data that was used to fit the model [see, e.g., @Claassen2019, 9].
In column 1, the mean absolute error (MAE) measures the average difference between the observed proportion of survey respondents in country $k$ in year $t$ with replies to question $q$ with a response at least as positive as response $r$ (or with an affirmative response, in the case of Claassen's [-@Claassen2019] Model 5) and the model's predicted proportion across all countries, years, questions, and response categories.
Given, however, that Claassen's [-@Claassen2019] Model 5 is fit to dichotomized data, while the @Caughey2019 and DCPO models are fit to the original, possibly ordinal, survey data, which have higher variance, comparing the MAE across all three models can be somewhat misleading.
Therefore, the MAE of the country means---the average proportions answering affirmatively (for Claassen's [-@Claassen2019] Model 5) or with a response at least as positive as response $r$ (for @Caughey2019 and DCPO) in each country across all years and all questions---serve as a baseline [see @Claassen2019, 11-12]; these appear in column 2.
The percentage reduction in MAE achieved by each model, listed in column 3, represents the improvement in fit compared to this baseline.

The last three columns of Table \ref{tab:validation_table} present the results of an external validation test, a test of the models' ability to predict out-of-sample survey responses.
This test employs $k$-fold validation with 10 folds, that is, it randomly divides the available country-year-questions into tenths and then sequentially treats each tenth as a test set to be predicted while fitting the model on a training set consisting of the other nine tenths of the data.
Column 4 shows the mean of the MAEs of the ten resulting sets of out-of-sample predictions, while column 5 presents the mean improvement over of these ten MAEs over their respective ten country-mean MAEs.
Column 6 reports the discrepancy between the percentage of all response proportions that fall within their corresponding predictions' 80% confidence intervals and the expected 80%; it therefore provides a gauge of the accuracy of the models' estimates of uncertainty.
A negative value in this column indicate that less than 80% of the observed out-of-sample survey proportions are included with the model's predictions' nominal 80% credible intervals, and so its uncertainty estimates are overconfident, while a positive value indicates the opposite, that more than 80% of the observed proportions fall within the credible intervals and so the model's uncertainty estimates are overly conservative.

```{r validation}
load(here::here("data", "validation_table.rda"))

validation_table
```

Comparing first the results for Claassen's [-@Claassen2019] Model 5 with those for the @Caughey2019 model reveals that, at least for this set of survey questions, while the former has a smaller MAE, the latter accounts for a larger percentage of the variation in its source data left unexplained by its respective country-means model.
This is true both in the internal validation test and on average across the ten folds of the external validation test.
Again, this discrepancy is possible due to the greater variance in the survey data when its ordinal nature is preserved, as in the @Caughey2019 model, rather than dichotomized, as in Claassen's [-@Claassen2019] approach.
An examination of column 6 reveals that the @Caughey2019 model yielded predictions with credible intervals that are much too narrow, encompassing only about 13% of the actual sample observations, while those for Claassen's [-@Claassen2019] Model 5 were slightly too conservative.^[
The relatively poor uncertainty estimates of the @Caughey2019 model are similar to those found for the dichotomous version presented in @Caughey2015 by @Claassen2019 [, 12].]

However, it is the DCPO model that provides what is unambiguously the best fit.
It features the smallest mean absolute error: on average, in the internal validation test its predictions are just over 3 percentage points away from the actual sample observations, and in the external validation test they are only 5.5 percentage points away from the actual out-of-sample observations.
In both cases, these represent the largest percentage improvement over the country-means MAE.
Further, in the external validation test, the DCPO credible intervals come closest to matching the nominal 80% level of any of the three models.


# Preparing and Using DCPO Estimates of Public Opinion

The old saw of data science---that at least 80\% of any project consists of wrangling the data [see, e.g., @Lohr2014]---applies with full force when conducting research on public opinion across countries and over time.
Fortunately, in addition to estimating the DCPO model, the `DCPO` package facilitates the formidable data management tasks involved: collecting and managing the survey datasets, identifying the country-years in which respondents were contacted in each, and preparing the response data for the model.^[
NB: At the moment, these functions are split out into [the `DCPOtools` package](https://github.com/fsolt/DCPOtools), but once `DCPOtools` is published to CRAN, `DCPO` will import it and incorporate its features.
This will be done before the paper goes out.]

To generate estimates using `DCPO`, one must first identify a list of questions that tap the desired aspect of public opinion, the order of their respective response categories, and the surveys in which they were asked.
This is an irreducibly labor-intensive task.^[
Triggering with a single click a series identical keyword searches across the major data archives should be possible, and would potentially speed this process at least a little, but I have not as yet pushed in this direction.]
But subsequent data managment chores are largely automated.

`DCPO` contains information on the country-years in which respondents were surveyed for over 600 survey datasets---users can easily add more---and it takes advantage of the work of @Arel-Bundock2018 to standardize the names of countries across datasets.
To ensure easy collaboration and replicability, the package automates, with a single command, the task of downloading needed survey datasets from the GESIS Data Catalogue [@Persson2015], the ICPSR Data Archive [@Solt2016b], the Pew Research Center [@Solt2016a], the Roper Center for Public Opinion Research [@Solt2017], the Dataverse Network [@Leeper2017], the European Social Survey [@Cimentada2019], and many other sources.
To be sure, there are many surveys for which downloads cannot be automated---for example the Asian Barometer, which is available only via email---but this step nevertheless greatly speeds the work of dataset acquisition.
An additional line of code generates the raw dataset: the weighted number of respondents in each country-year who provided each response to each question in each survey in the researcher's list.
One last function reformats the data as input to estimate the DCPO model.

After estimating the DCPO model, researchers should be aware that while such latent variable models seek to make the most of the data available, they cannot completely eradicate the issues of data scarcity and incomparability.
Rather, where and when data are more sparse, the credible intervals of the model estimates will---and should---expand to reflect our greater uncertainty as to the state of public opinion.
When using the resulting public opinion estimates in analyses, this uncertainty needs to be taken into account.
This means not simply employing the point estimates, that is, the means of the posterior distributions [_contra_ @Claassen2020a; @Claassen2020b].^[
The analysis in @OGrady2019 appears to do the same, but the replication materials for that article are as yet not accessible to confirm.]

Instead, researchers should follow the recommendations of work using other latent variables [e.g., @Schnakenberg2014; @Crabtree2015] and other measures incorporating uncertainty, such as the Standardized World Income Inequality Database [@Solt2020]: generate many duplicate versions of the analysis dataset, assign to each a different random draw from the posterior distributions of the variables measured with uncertainty, perform the analyses repeatedly on each of these multiple versions of the dataset, and combine the results following the rules set out in @Rubin1987.
The functional programming tools in the `purrr` package [@Henry2019] make this a matter of just a few additional lines of code.
Although it is intuitive that neglecting measurement error can lead to false positive results, the example presented by @Crabtree2015 demonstrates that doing so can lead to false negative results as well.
Quantifying the uncertainty in the measures is one of the strengths of this sort of latent variable estimation, and researchers should be sure to take advantage of it.

\vspace{30pt}
\noindent \textbf{[Could probably use an application here, especially one that shows off the value of having $\sigma_{k,t}$ as a measure of polarization in public opinion.]}

# Conclusions

The development of latent variable models that identify how public opinion varies both across countries and over time has the potential to trigger a new wave of research on the causes and consequences of public opinion that will take into account the experiences of many countries.
This will be crucial for developing and testing our understanding of, for example, not merely if policy change leads to changes in public opinion, but to the conditions in which accelerating, self-reinforcing, or self-undermining feedback [in the useful typology proposed by @Busemeyer2020] are more likely to occur.

By examining a broad sample of democracies, DCPO helps researchers avoid conclusions based on the idiosyncrasies of any given political setting and provide a firmer grounding for our understanding of how democracies work and the threats to representation that they face.

<!-- Re reproducibility: @Damian2019 -->

\pagebreak

