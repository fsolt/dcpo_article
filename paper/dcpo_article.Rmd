---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: false
    fig_caption: true
    latex_engine: pdflatex
    template: svm-latex-ms2.tex
title: "Modeling Dynamic Comparative Public Opinion [DRAFT]"
thanks: "The paper's revision history and the materials needed to reproduce its analyses can be found [on Github here](http://github.com/fsolt/dcpo_article).  I am grateful for comments received at the 2014 meetings of the European Political Science Association and American Political Science Association, the 2016 meeting of the Midwest Political Science Association, and at Princeton University, Oxford University, the University of Tennessee, the University of Iowa, Central European University, and the Polish Academy of Sciences.  Corresponding author: [frederick-solt@uiowa.edu](mailto:frederick-solt@uiowa.edu). Current version: `r format(Sys.time(), '%B %d, %Y')`."
author:
- name: Frederick Solt
  affiliation: University of Iowa
abstract: "The study of public opinion in comparative context has been hampered by data that is sparse, that is, unavailable for many countries and years; incomparable, i.e., ostensibly addressing the same issue but generated by different survey items; or, most often, both.  Questions of representation and of policy feedback on public opinion, for example, cannot be explored fully from a cross-national perspective without comparable time-series data for many countries that span their respective times of policy adoption.  Recent works (Claassen 2019; Caughey, O'Grady, and Warshaw 2019) have introduced a latent variable approach to the study of comparative public opinion that maximizes the information gleaned from available surveys to overcome issues of missing and incomparable data and allow comparativists to examine the dynamics of public opinion.  This paper advances this  field of research by presenting a new model and software for estimating latent variables of public opinion from cross-national survey data that yield superior fit and more quantities of theoretical interest than previous works allow."
keywords: "public opinion, item response theory, measurement"
date: "`r format(Sys.time(), '%B %d, %Y')`"
fontsize: 11pt
spacing: double
bibliography: \dummy{`r file.path(getwd(), list.files(getwd(), ".bib$"))`}
biblio-style: apsr
citecolor: black
linkcolor: black
endnote: no
header-includes:
      - \usepackage{array}
      - \usepackage{caption}
      - \usepackage{graphicx}
      - \usepackage{siunitx}
      - \usepackage{colortbl}
      - \usepackage{multirow}
      - \usepackage{hhline}
      - \usepackage{calc}
      - \usepackage{tabularx}
      - \usepackage{threeparttable}
      - \usepackage{wrapfig}
---

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
# load all the packages you will use below 
library(huxtable)
library(kableExtra)
library(DCPOtools)
library(tidyverse)
library(latex2exp)
```

A wealth of surveys provide information on the state of public opinion on various issues in different countries over the years, but scholars have faced significant hurdles to putting all of this information to use in any comparative study.
The most challenging of these obstacles is that, across countries and over time, the questions asked regarding any given issue are rarely the same, making responses to these questions incomparable.

As a result, the most common approach to the study of comparative public opinion is to use a single cross-section, typically provided by a single cross-national survey [see, e.g., @Dalton2011; @Ansell2014].
Some works have captured some element of change over time by taking advantage of multiple waves of an ongoing cross-national survey [see, e.g., @Inglehart1997; @Inglehart2005] or, more rarely, drawing on multiple surveys that employed the same item [see, e.g., @Solt2011; @Ezrow2014].

<!-- A growing body of work is taking a different tack, examining the dynamics of public opinion in single countries over time.  These studies draw on -->

<!-- (Not using stimson:  -->
<!-- @Hobolt2008  -->
<!-- Stubager2015) -->

<!-- dynamic comparative: Hagemann2016 -->

<!-- field of comparative public opinion -->
<!-- lack of dynamics (in contrast to public opinion work in U.S.) -->
<!-- Check out -->
<!-- 	Thomassen2011 -->
<!-- 	Russell Dalton's work -->
<!-- 	Norris on trust (2011, 63-77) -->


<!-- problem: 	scarce data -->
<!-- 		perennial problem of public opinion research: exact question wanted is rarely asked -->
<!-- 		no (good) way to (fully) integrate what does exist -->

<!-- question of feedback: positive or negative? ("policies create constituencies" vs. thermostatic) under what conditions? -->



<!-- (Treier and Jackman 2008; Pemstein, Meserve, and Melton 2010, [Arel-Bundock and Mebane 2011])  -->

# Estimating Dynamic Comparative Public Opinion

The logic underlying DCPO's approach to estimating dynamic comparative public opinion starts at the individual level with the two-parameter logistic (or "2PL") IRT model.  In this model, the probability that individual $i$ responds affirmatively to a dichotomous question $q$ is a function of the individual's score on the unbounded latent trait, $\theta'_{i}$, plus two parameters that characterize the question, its *difficulty*, $\beta_{q}$, and its *dispersion*, $\alpha_{q}$:
\begin{equation}
\textrm{Pr}(y_{iq} = 1) = \textrm{logit}^{-1}(\frac{\theta'_{i} - \beta_{q}}{\alpha_{q}}) \label{eq:irt}
\end{equation}

Figure \ref{fig:irt} shows how these parameters interact using simulated data.
First, both panels show that regardless of the question, as an individual's score on the unbounded latent trait, $\theta'_{i}$, increases, the individual becomes more likely to give an affirmative response.
Second, the left panel reveals that individuals with the same score on the latent trait are less likely to respond affirmatively to a question as the question's difficulty, $\beta_{q}$, increases; here, all three questions depicted have a dispersion of 1.
When $\theta'_{i}$ and $\beta_{q}$ are equal, the probability of an affirmative response is 50%. 
Third, the right panel shows that as the *dispersion* of the question, $\alpha_{q}$, which is proportional to its measurement error, increases, individuals with lower scores on the latent trait become more likely to respond affirmatively and those with higher scores on the latent trait are more likely to respond negatively; in other words, the slope of the curve describing the relationship between the latent trait and the probability of answering affirmatively flattens out.

```{r irt, fig.width=6, fig.height=3, fig.cap="\\label{fig:irt}Probability of an Affirmative Response in the 2PL IRT Model"}
tibble(theta = rep(seq(-3, 3, length.out = 100), 6),
       beta = rep(c(-2, 0, 2, 0, 0, 0), each = 100),
       alpha = rep(c(1, 1, 1, .25, 1, 2), each = 100),
       pr_y = plogis((theta - beta)/alpha),
       line_no = rep(1:6, each = 100),
       plot_facet = rep(c("Varying Question Difficulty", "Varying Question Dispersion"), each = 300)) %>% 
    ggplot(aes(theta, pr_y, group = line_no)) +
    geom_line() +
    facet_wrap( ~ plot_facet) +
    xlab(TeX("Individual Latent Trait")) +
    ylab(TeX("Pr(y_{iq} = 1)")) +
    theme_bw() +
    theme(strip.background = element_rect(colour="white", fill="white"),
          plot.title.position = "plot") +
    scale_x_continuous(breaks = seq(-3, 3, 1)) +
    geom_text(data = tibble(theta = -2.25,
                            pr_y = .65,
                            line_no = 1,
                            plot_facet = "Varying Question Difficulty",),
             label = TeX("$\\beta_1 = -2$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = -.6,
                            pr_y = .525,
                            line_no = 1,
                            plot_facet = "Varying Question Difficulty",),
              label = TeX("$\\beta_2 = 0$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = .9,
                            pr_y = .4,
                            line_no = 1,
                            plot_facet = "Varying Question Difficulty",),
              label = TeX("$\\beta_3 = 2$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = 2.5,
                            pr_y = 0.025,
                            line_no = 1,
                            plot_facet = "Varying Question Difficulty",),
              label = TeX("$\\alpha_q = 1$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = -.03,
                            pr_y = .96,
                            line_no = 1,
                            plot_facet = "Varying Question Dispersion",),
             label = TeX("$\\alpha_1 = .25$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = 1.4,
                            pr_y = .9,
                            line_no = 1,
                            plot_facet = "Varying Question Dispersion",),
              label = TeX("$\\alpha_2 = 1$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = 2.1,
                            pr_y = .66,
                            line_no = 1,
                            plot_facet = "Varying Question Dispersion",),
              label = TeX("$\\alpha_3 = 2$", output = "character"), parse = TRUE) +
    geom_text(data = tibble(theta = 2.5,
                            pr_y = 0.025,
                            line_no = 1,
                            plot_facet = "Varying Question Dispersion",),
              label = TeX("$\\beta_q = 0$", output = "character"), parse = TRUE)

```

To aggregate individual-level responses to generate a population-level estimate of public opinion is a matter of integration.
If scores on the unbounded latent trait are modeled as normally distributed within the population of country $k$ at time $t$, integrating Equation \ref{eq:irt} yields the population-level two-parameter logistic IRT model:
\begin{equation}
\eta_{ktq} = \textrm{logit}^{-1}(\frac{\bar{\theta'}_{kt} - \beta_{q}}{\sqrt{\alpha_{q}^2 + (1.7*\sigma_{kt})^2}}) \label{eq:pop_irt}
\end{equation}
where $\eta_{ktq}$ is the expected probability that a random person in country $k$ at time $t$ answers question $q$ affirmatively and $\bar{\theta'}_{kt}$ and $\sigma_{kt}$ are the mean and standard deviation of the unbounded latent trait $\theta'$ in the population of country $k$ at time $t$ [see @Mislevy1983, 280; see also, using the two-parameter probit IRT model, @McGann2014, 120].

Further, many survey questions are not simply dichotomous, but instead measure respondents' attitudes on ordinal scales, assumed here to range from 1 to $R$.
To take advantage of this additional information, DCPO uses the cumulative logit formulation, in which the probability to be estimated is not that individual $i$ provided a simple affirmative response, as in Equation \ref{eq:pop_irt} above, but instead the probability that individual $i$ provided a response at least as positive as response $r$ for all $r$ greater than 1 and less than or equal to $R$.
This yields a population-level graded response model:
\begin{equation}
\eta_{ktqr} = \textrm{logit}^{-1}(\frac{\bar{\theta'}_{kt} - \beta_{qr}}{\sqrt{\alpha_{q}^2 + (1.7*\sigma_{kt})^2}}) \label{eq:pop_grm}
\end{equation}

Again, the differences between Equation \ref{eq:pop_grm} for the population-level graded response model and Equation \ref{eq:pop_irt} for the population-level two-parameter logistic IRT model above are in the additional subscripts for $r$ to $\eta$ and $\beta$.
In Equation \ref{eq:pop_grm}, $\eta_{ktqr}$ is the expected probability that a random individual in country $k$ at time $t$ replies to question $q$ with a response at least as positive as response $r$.
And the additional subscript to $\beta_{qr}$ indicates that this parameter represents the difficulty of response $r$ of question $q$, constrained to be increasing for increasing $r$. 

A final addition to the DCPO model of the probability $\eta_{ktqr}$ takes into account differences in item response bias across countries.
Responses to survey questions may vary across different countries not only as a result of differences in attitudes and preferences but also due to translation issues [see, e.g., @Davidov2010], cultural differences in acquiescence and extreme response styles [see, e.g., @VanHerk2004], or other idiosyncrasies---recall Tarrow's [-@Tarrow1971, 344] famous observation that the French understood survey questions regarding their 'interest in politics' as inquiring about the strength of their partisan affiliations.
Rather than simply allowing such problems of equivalence to contribute to the error of the model, they can be addressed explicitly by modeling the country-specific item bias  [@Stegmueller2011], denoted here as $\delta_{kq}$.
In the context of the population-level graded response model presented in Equation \ref{eq:pop_grm}, including country-specific item bias can be readily understood as a country-varying shift in the difficulty of each question:
\begin{equation}
\eta_{ktqr} = \textrm{logit}^{-1}(\frac{\bar{\theta'}_{kt} - (\beta_{qr} + \delta_{kq})}{\sqrt{\alpha_{q}^2 + (1.7*\sigma_{kt})^2}}) \label{eq:dcpo}
\end{equation}
Estimating $\delta_{kq}$ in Equation \ref{eq:dcpo} requires repeated administrations of question $q$ in country $k$.
When responses to question $q$ are observed in country $k$ in only a single year, $\delta_{kq}$ is set to zero by assumption.
This means that incorporating these 'one-shot' surveys will come at the cost of increasing the error of the model by any country-item bias that is present.^[
_Pace_ Claassen's [-@Claassen2019, 5-6] approach, which is also to estimate country-specific item bias, but to discard survey data on questions that were not administered more than once in a given country.]
Of course, questions that are asked repeatedly over time in only a single country pose no risk of country-specific item bias, so $\delta_{kq}$ in such cases can be set to zero without harm.

Before continuing, it is useful to pause to briefly review the relationships of the parameters in this equation.
The numerator of Equation \ref{eq:dcpo} implies first that $\eta_{ktqr}$, the expected probability of a response at least as positive as $r$, increases as the mean value of the unbounded latent trait in the population, $\bar{\theta'}_{kt}$, increases and second that $\eta_{ktqr}$ decreases with more difficult questions and higher response categories, $\beta_{qr}$, and particularly so where the item-response bias, $\delta_{kq}$, is positive.
The equation's denominator implies that $\eta_{ktqr}$ is drawn closer to 50% as the geometric mean of the question's dispersion, $\alpha_{q}$, and the standard deviation in opinion in the population, $\sigma_{kt}$, increases: individuals sampled even from a population with a negative mean value on the unbounded latent trait and answering a difficult question will still be more likely to supply more positive responses---and those from a population with a positive mean score on the unbounded latent trait and answering an easy question will be more likely to supply more negative responses---if the question's dispersion is greater or attitudes are more polarized in the population.

Given this expected probability $\eta_{ktqr}$, the total number of survey responses at least as positive as $r$ to each question $q$ in country $k$ at time $t$, $y_{ktqr}$, out of the total number of respondents surveyed, $n_{ktqr}$, is then modeled using the beta-binomial distribution, which allows for an overall dispersion parameter, $\phi$, to account for additional sources of survey error [see @McGann2014, 120; @Claassen2019, 4-5].
\begin{equation}
a_{ktqr} = \phi\eta_{ktqr} \label{eq:bb_a}
\end{equation}
\begin{equation}
b_{ktqr} = \phi(1 - \eta_{ktqr}) \label{eq:bb_b}
\end{equation}
\begin{equation}
y_{ktqr} \sim \textrm{BetaBinomial}(n_{ktqr}, a_{ktqr}, b_{ktqr}) \label{eq:betabinomial}
\end{equation}

Then, to estimate the dynamics of comparative public opinion---the change over time---the prior distributions for the public opinion parameters in Equation \ref{eq:dcpo}, $\bar{\theta'}_{kt}$ and $\sigma_{kt}$, are given by simple local-level dynamic linear  models:
\begin{equation}
\bar{\theta}_{kt} \sim \textrm{N}(\bar{\theta'}_{k,t-1}, \sigma_{\bar{\theta'}}^2) \label{eq:prior_theta}
\end{equation}
\begin{equation}
\sigma_{kt} \sim \textrm{LN}(\sigma_{k,t-1}, \sigma_{\sigma}^2) \label{eq:prior_sigma}
\end{equation}
By treating these parameters' values at time $t-1$ as their expected values at time $t$, these priors work to smooth estimates of both the mean and the standard deviation of the latent trait in the population of each country over time.
If no survey data is available for a particular time, these models provide estimates based on the estimates for previous and subsequent periods.
The variances $\sigma_{\bar{\theta'}}^2$ and $\sigma_{\sigma}^2$ are estimated from the data.

Previous efforts to measure cross-national aggregate public opinion as a latent variable have generated estimates on an unbounded scale with mean zero and unit variance [see @Claassen2019, 14; @Caughey2019, 8], and this is the scale of $\bar{\theta'}_{kt}$.
However, like many other concepts in political science [see @Linzer2015, 229], many aspects of public opinion are conceptually bounded, that is, it make sense to think of them as lying along a scale from fully absent to fully present in the relevant public.
Take attitudes toward immigration as an example.
One hypothetical country's citizens are, at a given time, absolutely opposed to any immigration, while another's are completely welcoming to migrants.
Actual countries' levels of public opinion toward immigration are better understood not as unbounded but as falling somewhere between the bounds described by these two hypothetical countries.
It is not surprising, then, that earlier works aggregating public opinion within a single country presented results on bounded scales [see @Stimson1991; @McGann2014].
Even if the issue in question is less easily viewed as bounded, bounding is still a good idea because it reduces the uncertainty for the estimates for countries at the extremes, that is, those countries whose values should in fact be easier to estimate.
As Linzer and Staton [-@Linzer2015, 229] note, "bounding the latent variable may do little harm to the scale and produce more sensible estimates of uncertainty."
DCPO therefore uses the logistic function to transform the unbounded estimates, $\bar{\theta'}_{kt}$, to the unit interval:
\begin{equation}
\bar{\theta}_{kt} = \textrm{logit}^{-1}({\bar{\theta'}_{kt}} - 1)
\end{equation}
The resulting $\bar{\theta}_{kt}$ is the DCPO estimate of the mean public opinion for country $k$ at time $t$.
Together, $\sigma_{kt}$, which provides a measure of polarization in public opinion, and $\bar{\theta}_{kt}$ will typically be the main quantities of interest.

The model is identified by imposing a series of constraints that fix location, direction, and scale.
The dispersion parameters $\alpha$ are constrained to be positive---and all survey responses are coded to have the same polarity---to fix direction.
One specified difficulty parameter $\beta$ is set to a value of 1 to identify location, and, as mentioned previously, for each question $q$ the difficulties for increasing response values $r$ are constrained to be increasing.
To ensure identification, the sum of $\delta_{kq}$ across all countries $k$ is fixed to zero for each question $q$:
\begin{equation}
\sum_{k = 1}^K \delta_{kq} = 0
\end{equation}

Weakly informative priors are placed on most parameters.
The dispersion parameters $\alpha_{q}$ are drawn from standard half-normal prior distributions, that is, the positive half of N(0, 1).
The first difficulty parameters for each question, $\beta_{q1}$, are drawn from standard normal prior distributions, and the differences between $\beta$s for each $r$ for the same question $q$ are drawn from standard half-normal prior distributions.
The item-bias parameters $\delta_{kq}$ receive normally-distributed hierarchical priors with mean 0 and standard deviations drawn from standard half-normal prior distributions.
The initial value of the mean unbounded latent trait for each country, $\bar{\theta'}_{k1}$, is assigned a standard normal prior, as are the transition variances $\sigma_{\bar{\theta'}}^2$ and $\sigma_{\sigma}^2$; the initial value of the standard deviation of the unbounded latent trait for each country, $\sigma_{k1}$, is drawn from a standard lognormal prior distribution.
The overall dispersion, $\phi$, receives a more informative prior drawn from a gamma(4, 0.1) distribution that yields values that are well scaled for that parameter.

```{r comparison_table}
comparison_data <- tibble::tribble(
    ~` `, ~`McGann (2014)`, ~`\\vtop{\\hbox{\\strut Claassen (2019)}\\hbox{\\strut \\quad \\, Model 5}}`, ~`\\vtop{\\hbox{\\strut Caughey, O'Grady,}\\hbox{\\strut and Warshaw (2019)}}`, ~DCPO,
    "\\vtop{\\hbox{\\strut Cross-National}\\hbox{\\strut }}", "No", "Yes", "Yes", "Yes",
    "\\vtop{\\hbox{\\strut Ordinal}\\hbox{\\strut }}", "No", "No", "Yes", "Yes",
    "\\vtop{\\hbox{\\strut Country-Varying}\\hbox{\\strut Question Difficulty}}", "No", "Yes", "No", "Yes",
    "\\vtop{\\hbox{\\strut Bounded Mean}\\hbox{\\strut Public Opinion}}", "Yes", "No", "No", "Yes",
    "\\vtop{\\hbox{\\strut Country-Year Public}\\hbox{\\strut Opinion Std.~Deviation}}", "Yes", "No", "No", "Yes"
)

comparison_ht <- comparison_data %>%
  as_hux() %>% 
  add_colnames() %>% 
  set_bold(1, 1:5, TRUE) %>% 
  set_bold(everywhere, final(1), TRUE) %>% 
  set_top_border(1, everywhere, 1) %>%
  set_bottom_border(1, 1:5, 1) %>%
  set_bottom_border(final(1), everywhere, 1) %>% 
  set_caption('Comparing Models of Public Opinion') %>% 
  set_position("left") %>% 
  set_escape_contents(everywhere, everywhere, FALSE) %>% 
  set_background_color(evens, everywhere, "grey95") %>% 
  set_align(-1, -1, "center") %>% 
  set_label("tab:comparison_table")

comparison_ht

```

Table \ref{tab:comparison_table} summarizes the DCPO model just described in comparison to three alternate ways of modeling public opinion: that presented by @McGann2014, Claassen's [-@Claassen2019] preferred Model 5, and that employed in @Caughey2019.
It reveals that DCPO incorporates the desirable features from each of those three models.
DCPO generates estimates of public opinion that are comparable not only over time but also across countries, takes advantage of all of the information encoded in responses to ordinal questions, explicitly accounts for country-item bias by including terms to capture country-varying question difficulty, provides easily interpreted bounded estimates of mean public opinion, and includes the standard deviation of public opinion by country-year to represent the extent of polarization.
The next section provides a demonstration of how these features allow DCPO to provide a better fit to actual data.

# Relative Performance
To assess the ability of DCPO to fit public opinion data relative to alternative approaches, I use the set of survey questions on support for democracy employed in @Claassen2019 [, 7-8].


The first three columns of Table \ref{tab:validation_table} present the results of the internal validation test, that is, a test that uses the same data that was used to fit the model [see, e.g., @Claassen2019, 9].
The mean absolute error (MAE) measures the average difference between the observed proportion of survey respondents in country $k$ in year $t$ with replies to question $q$ with a response at least as positive as response $r$ (or with an affirmative response, in the case of Claassen's [-@Claassen2019] Model 5) and the model's predicted proportion across all countries, years, questions, and response categories.
Given, however, that Claassen's [-@Claassen2019] Model 5 is fit to dichotomized data, while the @Caughey2019 and DCPO models are fit to the original, possibly ordinal, survey data, which have higher variance, comparing the MAE across all three models can be somewhat misleading.
Therefore, the MAE of the country means---the average proportions answering affirmatively (for Claassen's [-@Claassen2019] Model 5) or with a response at least as positive as response $r$ (for @Caughey2019 and DCPO) in each country across all years and all questions---serve as a baseline [see @Claassen2019, 11-12].
The percentage reduction in MAE achieved by each model represents the improvement in fit compared to this baseline.

The last three columns of Table \ref{tab:validation_table} present the results of an external validation test, a test of the models' ability to predict out-of-sample survey responses. 
This test employs $k$-fold validation with 10 folds, that is, it randomly divides the available country-year-questions into tenths and then sequentially treats each tenth as a test set to be predicted while fitting the model on a training set consisting of the other nine tenths of the data.
The fourth column shows the mean of the MAEs of the ten sets of out-of-sample predictions, the fifth presents the mean improvement over of these ten MAEs over their respective ten country-mean MAEs, and the sixth reports the discrepancy between the percentage of all response proportions that fall within their corresponding predictions' 80% confidence intervals and the expected 80%.

```{r validation}
load(here::here("data", "validation_table.rda"))

validation_table
```

Comparing first the results for Claassen's [-@Claassen2019] Model 5 with those for the @Caughey2019 model reveals that, for this set of survey questions, while the former has a smaller MAE, the latter accounts for a larger percentage of the variation in its source data unexplained by the country-means model.
Again, this discrepancy is possible due to the greater variance in the survey data when its ordinal nature is preserved rather than dichotomized.
However, the DCPO model provides what is unambiguously the best fit to these data, with the smallest MAE and the largest percentage improvement over the country-means MAE.

# Conclusions
<!-- strengths: -->
<!--     makes maximum use of available data -->
<!-- 	permits testing hypotheses regarding change over time -->
<!-- 	incorporates uncertainty -->

<!-- weaknesses: -->
<!-- 	data collection demands -->
<!-- 	challenges at individual level (but subsets) -->

Single-country studies of public opinion have flourished since the release of Stimson's [-@Stimson1991] algorithm for identifying the common trends in any collection of survey questions that have been repeatedly asked over many years.
By extending this work to allow the creation of cross-national pooled time series that identify how public opinion varies both across countries and over time, DCPO has the potential to trigger a new wave of research on the causes and consequences of public opinion that will take into account the experiences of many countries.

Further, this allows a broadly comparative approach that is new to work on the relationship between opinion and policy.
Existing studies on that examine this topic over time investigate only a single country or, much more rarely, a handful of countries.
By examining a broad sample of democracies, DCPO helps researchers avoid conclusions based on the idiosyncrasies of any given political setting and provide a firmer grounding for our understanding of how democracies work and the threats to representation that they face.


<!-- Re reproducibility: @Damian2019 -->

\pagebreak

